
```` Key Concepts and Practices in Software Testing Life Cycle (STLC) ````

================================================================================

--- Core STLC Concepts ---

================================================================================
@Definition :
    Software Testing Life Cycle (STLC) is a sequence of phases followed to test software systematically and ensure its quality before release.

STLC Stages (Brief):
    - Requirement Analysis: Understand and analyze testable requirements.    
    - Test Planning: Prepare the test strategy, scope, and schedule.
    - Test Case Design: Create test cases and scripts.
    - Test Environment Setup: Configure the environment and tools for testing.
    - Test Execution: Run tests and record results.
    - Defect Reporting: Log and track bugs found.
    - Test Closure: Review testing, report results, and document lessons learned.
    - Retesting & Regression Testing: Verify fixes and check for new issues.
================================================================================

--- Testing Techniques & Strategies ---

================================================================================
Q. What is verification and validation in testing?  
Ans.) Verification checks if the product is being built correctly by reviewing documents, designs, and processes. Validation ensures the final product works as expected for the user by executing tests on the actual software.
================================================================================
Q. What is boundary value analysis (BVA)? 
Ans.) BVA is a test case design technique that chooses test data at the edges of input ranges where errors are most likely to occur. It includes values just below, at, and just above the boundaries to ensure the system handles edge cases correctly.
================================================================================
Q.) What is Boundary Testing?
Ans.) Boundary Testing is a black-box testing technique that focuses on testing the input values at the edges or boundaries of valid input ranges. It checks values like minimum, maximum, just below minimum, and just above maximum to detect errors at the limits.
================================================================================
Q. What is equivalence partitioning?  
Ans.) Equivalence Partitioning is a black-box testing technique that divides input data into partitions or classes where all values within each partition are expected to produce similar behavior. These partitions include valid and invalid input ranges. Testing a single value from each partition is considered sufficient, as it is assumed to represent the behavior of the entire group.
================================================================================
Q. What is exploratory testing?  
Ans.) Exploratory testing combines learning, test design, and execution simultaneously. It relies on the tester's experience and creativity to find unexpected issues.
================================================================================
Q. What is error guessing?  
Ans.) Error guessing is a technique where testers anticipate potential defects based on past experience. It involves creating test cases around common or historically problematic areas in the application.
================================================================================
Q. What is the difference between blackbox and whitebox testing?  
Ans.) Blackbox testing checks application functionality without knowing its internal code. Whitebox testing is based on code structure and logic, ensuring internal components behave correctly.
================================================================================ 
Q. What is Branch Testing?
Ans.) Branch Testing is a white-box testing technique where each possible branch or decision in the code is tested at least once. It ensures that both true and false outcomes of conditions (like if-else) are covered, helping to find logical errors in decision-making.
================================================================================

--- Test Management & Planning ---

================================================================================
Q. What is a test plan?  
Ans.) A Test Plan is a formal document that outlines the scope, objectives, resources, schedule, and approach for testing a project. It serves as a roadmap to guide the testing team and ensure testing is aligned with business goals and carried out systematically.
   
It typically includes:
    - Test objectives : What needs to be tested and goals.
    - Scope : Features to be tested and out of scope.
    - Test approach : Testing types and methods used.
    - Test environment : Hardware, software, and tools required.
    - Test schedule : Timeline and milestones.
    - Resource allocation : Team roles and responsibilities.
    - Entry and exit criteria : Conditions to start and stop testing.
    - Risks and contingencies : Potential issues and mitigation plans.
    - Deliverables : Test cases, reports, logs, etc.
================================================================================
Q. What is RTM (Requirement Traceability Matrix)?  
Ans.) RTM maps requirements to corresponding test cases to ensure complete test coverage. It helps track changes and ensures that no requirement is left untested.
================================================================================
Q. What is a test closure report?  
Ans.) It summarizes all testing activities, results, defects, and quality metrics at the end of the testing cycle. The report provides insights, lessons learned, and recommendations for future projects.
================================================================================
Q. What reports are used in testing?  
Ans.) Common reports include Test Summary, Bug Reports, Execution Reports, Coverage Reports, and RTM. These documents help track testing progress, defect status, and overall product quality.
================================================================================

--- Defects & Prioritization ---

================================================================================
Q. How do you prioritize test cases?  
Ans.) Test cases are prioritized based on risk, requirement criticality, severity, and business needs. Higher priority is given to test cases affecting critical features or areas prone to failure.
================================================================================
Q. Define severity and priority.  
Ans.) Severity reflects the impact of a defect on the application’s functionality.  
    Priority indicates how soon the defect should be fixed from a business perspective.
================================================================================
Q. What is a showstopper defect?  
Ans.) A showstopper defect is a critical issue that blocks further testing or usage of the application. Testing is suspended until the defect is resolved, as it hinders core functionality.
================================================================================

--- Types of Testing ---

================================================================================
Q. What is smoke vs sanity testing?  
Ans.) Smoke Testing is a preliminary test to check whether the basic and critical functionalities of a software build are working. It's often referred to as a "build verification test" and is performed before more in-depth testing begins. If the smoke test fails, the build is rejected.

Sanity Testing is a narrow and focused test conducted after receiving a software build with minor changes or bug fixes. It ensures that the specific functionalities or fixes work correctly and haven't adversely affected other parts of the application.
================================================================================
Q. What is regression testing?  
Ans.) Regression testing checks if recent code changes have broken any existing features. It helps make sure that the old parts of the software still work as expected after updates or bug fixes. These tests are done again and again for every new build to keep the software stable.
================================================================================
Q. What is retesting?  
Ans.) Retesting involves executing failed test cases after defects are fixed to confirm they are resolved. It is done on the same environment using the same inputs to ensure accuracy.
================================================================================
Q. What is integration testing?  
Ans.) Integration testing checks if different parts (or modules) of the software work properly when combined. It makes sure they talk to each other correctly and share data the right way. It helps find problems in how the modules connect and interact.
================================================================================
Q. What is topdown vs bottomup testing?  
Ans.)
Top-Down Integration Testing:
    - Testing starts from the top (main modules) and moves downward to smaller parts.
    - Big modules are tested first, and smaller ones are added step by step.
    - If lower modules aren't ready, stubs (fake modules) are used to act like them.
    Advantage: You can test main logic and design early.
    Disadvantage: Smaller parts are tested late, and making stubs can be tricky.

Bottom-Up Integration Testing:
    - Testing starts from the bottom (small modules) and goes upward to the main modules.
    - Small parts are tested first, then connected to bigger ones.
    - If top modules aren’t ready, drivers (test programs) are used to call them.
    Advantage: Small, important pieces are tested early.
    Disadvantage: You see the full system working only at the end.
================================================================================
Q. What is big bang testing?  
Ans.) In big bang testing, all components are integrated at once and tested collectively. It can be risky as defects are harder to isolate and may impact multiple areas.
================================================================================

--- Special Testing Types ---

================================================================================
Q. What is A/B testing?  
Ans.) A/B testing compares two versions of a product to evaluate which one performs better with users. It helps optimize UI/UX or feature effectiveness based on realworld feedback.
================================================================================
Q. What is concurrency testing?  
Ans.) Concurrency testing verifies the application can handle multiple operations simultaneously. It checks for issues like deadlocks, race conditions, or performance degradation.
================================================================================
Q. What is interface testing?  
Ans.) It tests the data flow and interaction between client, server, and database components. The goal is to ensure correct communication and data handling across layers.
================================================================================
Q. What is monkey, gorilla, and adhoc testing?  
Ans.)  Monkey Testing: Random testing without predefined inputs or logic.  
     Gorilla Testing: Repeatedly testing one functionality heavily to check its robustness.  
     Adhoc Testing: Unstructured testing to explore and find defects intuitively.
================================================================================

--- Advanced Concepts ---

================================================================================
Q. What is shiftleft testing?  
Ans.) Shiftleft testing means involving testing activities earlier in the development process. It leads to early bug detection, better collaboration, and reduced cost of fixes.
================================================================================
Q. What are challenges in STLC?  
Ans.) Challenges include unclear requirements, communication gaps, resource constraints, and tool compatibility issues. They can affect test coverage, execution speed, and overall product quality.
================================================================================
Q. How do you integrate automation in STLC?  
Ans.) Select repetitive and high risk test cases, then automate them using suitable tools and frameworks. Integrate tests into CI/CD for continuous execution and faster feedback.
================================================================================
Q. How do you handle flaky tests?  
Ans.) Analyze flaky tests for timing, environment, or data issues and stabilize them using waits or test isolation. Regular reviews and root cause analysis help maintain test reliability.
================================================================================
Q. What is error seeding?  
Ans.) Error seeding involves deliberately inserting known defects into the system. It helps measure the effectiveness of the testing process and testers’ ability to detect bugs.
================================================================================
Q. What is risk mitigation in testing?  
Ans.) Risk mitigation identifies potential risks and applies controls to reduce their impact. This ensures testing efforts are focused and resources are efficiently utilized.
================================================================================
Q. What are key terms in software testing?  
Ans.) 
1. Test Scenario – High-level condition to be tested
2. Test Case – Step-by-step instructions to verify functionality
3. Test Suite – Group of test cases for a module
4. Test Data – Input values used during testing
5. Test Plan – Strategy, scope, and schedule of testing
6. RTM – Mapping of requirements to test cases
7. Functional Testing – Validates expected functionality
8. Non-Functional Testing – Checks performance, security, usability
9. Exploratory Testing – Learning + testing without scripts
10. Smoke Testing – Basic build verification
11. Sanity Testing – Quick checks for specific areas
12. Regression Testing – Ensure new code doesn’t break existing features
13. Retesting – Testing failed test cases after bug fix
14. Defect Life Cycle – Bug journey from open to close
15. Severity – Impact of a defect on the system
16. Priority – Urgency of fixing the defect
17. BVA – Testing at input boundaries
18. EP – Grouping inputs into valid/invalid sets
19. UAT – End-user validation of requirements
20. QA – Process to prevent defects
21. QC – Finding defects in the product
22. Bug Leakage – Bugs missed in testing, found later
23. Bug Release – Known bugs released intentionally
24. Automation Testing – Using tools to auto-execute tests
25. CI/CD – Automated integration and deployment
26. Assertions – Validating expected vs actual result
27. API Testing – Testing backend using tools like Postman
28. Load Testing – Testing under expected user load
29. Integration Testing – Checking interaction between modules
30. E2E Testing – Verifying complete user flow.
